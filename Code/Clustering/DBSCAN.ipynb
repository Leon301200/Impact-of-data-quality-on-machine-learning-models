{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-21T13:10:25.716452Z",
     "start_time": "2024-08-21T13:10:25.711577Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_mutual_info_score, silhouette_score, adjusted_rand_score, \\\n",
    "    normalized_mutual_info_score\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:10:26.462618Z",
     "start_time": "2024-08-21T13:10:26.445937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_csv_files_to_dict(base_path, dimension, file_prefix, missing_percentages, strategies=None):\n",
    "    dataframes_dict = {}\n",
    "\n",
    "    for percentage in missing_percentages:\n",
    "        percentage_key = f\"{int(percentage * 100)}%\"\n",
    "\n",
    "        if dimension == 'Completeness':\n",
    "            dataframes_dict[percentage_key] = {}\n",
    "\n",
    "            if strategies is not None:\n",
    "                for strategy_name, strategy_func, subfolder in strategies:\n",
    "                    file_path = f\"{base_path}/{dimension}/{subfolder}/{file_prefix}_{percentage_key}.csv\"\n",
    "\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        dataframes_dict[percentage_key][strategy_name] = df\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                        dataframes_dict[percentage_key][strategy_name] = None\n",
    "            else:\n",
    "                print(\"No strategies provided for Completeness dimension.\")\n",
    "        elif dimension == 'Unicity':\n",
    "            file_path = f\"{base_path}/{dimension}/{file_prefix}_{percentage_key}_2x.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dataframes_dict[percentage_key] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                dataframes_dict[percentage_key] = None\n",
    "        else:\n",
    "            file_path = f\"{base_path}/{dimension}/{file_prefix}_{percentage_key}.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dataframes_dict[percentage_key] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                dataframes_dict[percentage_key] = None\n",
    "\n",
    "    return dataframes_dict\n",
    "\n",
    "def convert_to_float(obj):\n",
    "    \"\"\"\n",
    "    Convertit les types de données non JSON-serializables en types natifs Python.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "def update_json_results(output_path, model_name, metrics, pollution_percentage_levels):\n",
    "    # Charger le fichier JSON existant, ou initialiser une nouvelle structure si le fichier n'existe pas\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as json_file:\n",
    "            results_dict = json.load(json_file)\n",
    "    else:\n",
    "        results_dict = {\n",
    "            \"models\": []\n",
    "        }\n",
    "\n",
    "    # Trouver ou ajouter l'entrée pour le modèle spécifié\n",
    "    model_entry = next((model for model in results_dict[\"models\"] if model[\"model\"] == model_name), None)\n",
    "\n",
    "    if not model_entry:\n",
    "        model_entry = {\n",
    "            \"model\": model_name,\n",
    "            \"pollution_metrics\": []\n",
    "        }\n",
    "        results_dict[\"models\"].append(model_entry)\n",
    "\n",
    "    # Mise à jour pour la pollution à 0 (df_clean)\n",
    "    pollution_percentage = 0\n",
    "    existing_entry = next((item for item in model_entry[\"pollution_metrics\"] if item[\"pollution_percentage\"] == pollution_percentage), None)\n",
    "\n",
    "    if existing_entry:\n",
    "        # Remplacer les métriques\n",
    "        existing_entry[\"metrics\"] = {\n",
    "            # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Clean\"]),\n",
    "            \"stability indexes\": 1.0,\n",
    "            \"ARI score\": 1.0,\n",
    "            \"Cluster_Overlap_Size\": 1.0,\n",
    "            \"Cluster_Size_Variance\": 1.0\n",
    "        }\n",
    "    else:\n",
    "        # Ajouter une nouvelle entrée pour ce pourcentage\n",
    "        model_entry[\"pollution_metrics\"].append({\n",
    "            \"pollution_percentage\": pollution_percentage,\n",
    "            \"metrics\": {\n",
    "                # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Clean\"]),\n",
    "                \"stability indexes\": 1.0,\n",
    "                \"ARI score\": 1.0,\n",
    "                \"Cluster_Overlap_Size\": 1.0,\n",
    "                \"Cluster_Size_Variance\": 1.0\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Mise à jour pour les autres niveaux de pollution\n",
    "    for i, pollution_percentage in enumerate(pollution_percentage_levels):\n",
    "        pollution_percentage = int(pollution_percentage*100)\n",
    "        existing_entry = next((item for item in model_entry[\"pollution_metrics\"] if item[\"pollution_percentage\"] == pollution_percentage), None)\n",
    "\n",
    "        if existing_entry:\n",
    "            # Remplacer les métriques\n",
    "            existing_entry[\"metrics\"] = {\n",
    "                # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Noisy\"][i]),\n",
    "                \"stability indexes\": convert_to_float(metrics[\"Stability_Index\"][i]),\n",
    "                \"ARI score\": convert_to_float(metrics[\"ARI\"][i]),\n",
    "                \"Cluster_Overlap_Size\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i]),\n",
    "                \"Cluster_Size_Variance\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i])\n",
    "\n",
    "            }\n",
    "        else:\n",
    "            # Ajouter une nouvelle entrée pour ce pourcentage\n",
    "            model_entry[\"pollution_metrics\"].append({\n",
    "                \"pollution_percentage\": pollution_percentage,\n",
    "                \"metrics\": {\n",
    "                    # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Noisy\"][i]),\n",
    "                    \"stability indexes\": convert_to_float(metrics[\"Stability_Index\"][i]),\n",
    "                    \"ARI score\": convert_to_float(metrics[\"ARI\"][i]),\n",
    "                    \"Cluster_Overlap_Size\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i]),\n",
    "                    \"Cluster_Size_Variance\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i])\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Écrire les résultats mis à jour dans le fichier JSON\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(results_dict, json_file, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "\n",
    "def prepare_retail_data(df):\n",
    "    # Séparation des caractéristiques numériques et catégorielles\n",
    "    # df = df.sample(frac=0.5, random_state=42)\n",
    "    \n",
    "    numeric_features = df.select_dtypes(include='number')\n",
    "    categorical_features = df.select_dtypes(include='object')\n",
    "\n",
    "    # Transformation des données numériques\n",
    "    scaler = StandardScaler()\n",
    "    numeric_data_scaled = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Transformation des données catégorielles en utilisant pd.get_dummies\n",
    "    categorical_data_encoded = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "    # Combinaison des données numériques et catégorielles transformées\n",
    "    df_preprocessed = pd.concat([pd.DataFrame(numeric_data_scaled, columns=numeric_features.columns).reset_index(drop=True),\n",
    "                                 categorical_data_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df_preprocessed\n",
    "\n",
    "def compute_metrics(df_clean, df_noisy_dict, clustering_algo=DBSCAN(eps=0.5, min_samples=5)):\n",
    "    metrics_results = {\n",
    "        \"Stability_Index\": [],\n",
    "        \"ARI\": [],\n",
    "        \"Cluster_Overlap_Size\": [],\n",
    "        \"Cluster_Size_Variance\": []\n",
    "    }\n",
    "\n",
    "    # 1. Réduction de dimensionnalité sur les données propres avec un nouveau modèle UMAP\n",
    "    print(\"Réduction de dimensionnalité sur les données propres\")\n",
    "    umap_model_clean = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "    embedding_clean = umap_model_clean.fit_transform(df_clean)\n",
    "\n",
    "    # 2. Clustering sur les données propres\n",
    "    print(\"Clustering sur les données propres\")\n",
    "    cluster_labels_clean = clustering_algo.fit_predict(embedding_clean)\n",
    "\n",
    "    for key in df_noisy_dict.keys():\n",
    "        df_noisy = df_noisy_dict[key]\n",
    "\n",
    "        if df_noisy is not None:\n",
    "            if len(df_noisy) > len(df_clean):\n",
    "                df_noisy_sampled = df_noisy.sample(n=len(df_clean), random_state=42)\n",
    "            else:\n",
    "                df_noisy_sampled = df_noisy\n",
    "\n",
    "            # 3. Réduction de dimensionnalité sur les données bruitées\n",
    "            print(f\"Réduction de dimensionnalité sur les données bruitées: {key}\")\n",
    "            umap_model_noisy = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "            embedding_noisy = umap_model_clean.fit_transform(df_noisy_sampled)\n",
    "\n",
    "            # 4. Clustering sur les données bruitées\n",
    "            print(f\"Clustering sur les données bruitées: {key}\")\n",
    "            cluster_labels_noisy = clustering_algo.fit_predict(embedding_noisy)\n",
    "\n",
    "            # 5. Calcul des métriques\n",
    "\n",
    "            # Adjusted Rand Index (ARI)\n",
    "            print(\"Calcul du Adjusted Rand Index (ARI) sur les données bruitées\")\n",
    "            ari = adjusted_rand_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"ARI\"].append(ari)\n",
    "\n",
    "            # Stability Index (SI)\n",
    "            print(\"Calcul du Stability Index (SI) sur les données bruitées\")\n",
    "            stability_index = normalized_mutual_info_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"Stability_Index\"].append(stability_index)\n",
    "\n",
    "            # Cluster Overlap Size\n",
    "            print(\"Calcul de la taille absolue de l'overlap des clusters\")\n",
    "            overlap_size = compute_cluster_overlap(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"Cluster_Overlap_Size\"].append(overlap_size)\n",
    "\n",
    "            # Cluster Size Variance\n",
    "            print(\"Calcul de la variance de la taille des clusters\")\n",
    "            size_variance = compute_cluster_size_variance(cluster_labels_noisy)\n",
    "            metrics_results[\"Cluster_Size_Variance\"].append(size_variance)\n",
    "        else:\n",
    "            metrics_results[\"ARI\"].append(None)\n",
    "            metrics_results[\"Stability_Index\"].append(None)\n",
    "            metrics_results[\"Cluster_Overlap_Size\"].append(None)\n",
    "            metrics_results[\"Cluster_Size_Variance\"].append(None)\n",
    "\n",
    "    return metrics_results\n",
    "\n",
    "\n",
    "def compute_cluster_overlap(labels1, labels2):\n",
    "    overlap = 0\n",
    "    for cluster1 in np.unique(labels1):\n",
    "        for cluster2 in np.unique(labels2):\n",
    "            intersection = np.sum((labels1 == cluster1) & (labels2 == cluster2))\n",
    "            union = np.sum(labels1 == cluster1) + np.sum(labels2 == cluster2) - intersection\n",
    "            if union > 0:\n",
    "                overlap += intersection / union\n",
    "    return overlap\n",
    "\n",
    "def compute_cluster_size_variance(labels):\n",
    "    sizes = [np.sum(labels == label) for label in np.unique(labels)]\n",
    "    return np.var(sizes) if len(sizes) > 1 else 0"
   ],
   "id": "eeaf5ec5b5f39213",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-21T03:23:51.231287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "missing_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "# Strategy :\n",
    "strategy_delete = {\n",
    "    'numerical': 'delete',\n",
    "    'categorical': 'delete'\n",
    "}\n",
    "\n",
    "strategy_mean_mode = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "strategy_median_new = {\n",
    "    'numerical': 'median',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_decision_tree = {\n",
    "    'numerical': 'decision_tree',\n",
    "    'categorical': 'decision_tree'\n",
    "}\n",
    "\n",
    "strategy_mean_new = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_knn_mode = {\n",
    "    'numerical': 'knn',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "strategies = [\n",
    "    (\"Mean and Mode\", strategy_mean_mode, \"Mean and Mode\"),\n",
    "    (\"Median and New\", strategy_median_new, \"Median and New\"),\n",
    "    (\"Decision Tree\", strategy_decision_tree, \"Decision Tree\"),\n",
    "    (\"Mean and New\", strategy_mean_new, \"Mean and New\")\n",
    "]\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement du DataSet bruité\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Completeness', 'retail', missing_percentages, strategies)\n",
    "\n",
    "for key, retail_df_strategies in retail_df_noisy_dict.items():\n",
    "    for strategies , retail_df in retail_df_strategies.items():\n",
    "        retail_df_noisy_dict[key][strategies] = prepare_retail_data(retail_df_noisy_dict[key][strategies])\n",
    "\n",
    "\n",
    "# Chemin du fichier JSON où les résultats seront enregistrés\n",
    "output_path = \"../../Results/Retail/Completeness.json\"\n",
    "\n",
    "# # Calcule des metriques\n",
    "for key, retail_df_strategies in retail_df_noisy_dict.items():\n",
    "    print(key)\n",
    "    metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict[key])\n",
    "    print(metrics)"
   ],
   "id": "91ec2f062d7669c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Accuracy",
   "id": "1a2e8445ed9b5d3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T23:47:43.826542Z",
     "start_time": "2024-08-20T23:36:52.710303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Feature Accuracy.json\"\n",
    "model_name = \"DBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Feature Accuracy', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "2112385c1bcb5a87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:31:10.237548Z",
     "start_time": "2024-08-21T13:11:15.851664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Consistent Representation.json\"\n",
    "model_name = \"DBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Consistent Representation', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "e91388bd48ad879f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: 10%\n",
      "Clustering sur les données bruitées: 10%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 20%\n",
      "Clustering sur les données bruitées: 20%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 30%\n",
      "Clustering sur les données bruitées: 30%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 40%\n",
      "Clustering sur les données bruitées: 40%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 50%\n",
      "Clustering sur les données bruitées: 50%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 60%\n",
      "Clustering sur les données bruitées: 60%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 70%\n",
      "Clustering sur les données bruitées: 70%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 80%\n",
      "Clustering sur les données bruitées: 80%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Results saved to ../../Results/Retail/Consistent Representation.json\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:43:24.117123Z",
     "start_time": "2024-08-21T13:31:10.241532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Unicity.json\"\n",
    "model_name = \"DBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Unicity', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "c228f2039e7b84c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: 10%\n",
      "Clustering sur les données bruitées: 10%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 20%\n",
      "Clustering sur les données bruitées: 20%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 30%\n",
      "Clustering sur les données bruitées: 30%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 40%\n",
      "Clustering sur les données bruitées: 40%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 50%\n",
      "Clustering sur les données bruitées: 50%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 60%\n",
      "Clustering sur les données bruitées: 60%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 70%\n",
      "Clustering sur les données bruitées: 70%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 80%\n",
      "Clustering sur les données bruitées: 80%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Results saved to ../../Results/Retail/Unicity.json\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d24772fea404184"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
