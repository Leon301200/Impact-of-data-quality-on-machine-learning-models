{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-21T13:12:09.572098Z",
     "start_time": "2024-08-21T13:12:09.543638Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_mutual_info_score, silhouette_score, adjusted_rand_score, \\\n",
    "    normalized_mutual_info_score\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:12:09.927088Z",
     "start_time": "2024-08-21T13:12:09.897713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_csv_files_to_dict(base_path, dimension, file_prefix, missing_percentages, strategies=None):\n",
    "    dataframes_dict = {}\n",
    "\n",
    "    for percentage in missing_percentages:\n",
    "        percentage_key = f\"{int(percentage * 100)}%\"\n",
    "\n",
    "        if dimension == 'Completeness':\n",
    "            dataframes_dict[percentage_key] = {}\n",
    "\n",
    "            if strategies is not None:\n",
    "                for strategy_name, strategy_func, subfolder in strategies:\n",
    "                    file_path = f\"{base_path}/{dimension}/{subfolder}/{file_prefix}_{percentage_key}.csv\"\n",
    "\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        dataframes_dict[percentage_key][strategy_name] = df\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                        dataframes_dict[percentage_key][strategy_name] = None\n",
    "            else:\n",
    "                print(\"No strategies provided for Completeness dimension.\")\n",
    "        elif dimension == 'Unicity':\n",
    "            file_path = f\"{base_path}/{dimension}/{file_prefix}_{percentage_key}_2x.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dataframes_dict[percentage_key] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                dataframes_dict[percentage_key] = None\n",
    "        else:\n",
    "            file_path = f\"{base_path}/{dimension}/{file_prefix}_{percentage_key}.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dataframes_dict[percentage_key] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                dataframes_dict[percentage_key] = None\n",
    "\n",
    "    return dataframes_dict\n",
    "\n",
    "def convert_to_float(obj):\n",
    "    \"\"\"\n",
    "    Convertit les types de données non JSON-serializables en types natifs Python.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "def update_json_results(output_path, model_name, metrics, pollution_percentage_levels):\n",
    "    # Charger le fichier JSON existant, ou initialiser une nouvelle structure si le fichier n'existe pas\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'r') as json_file:\n",
    "            results_dict = json.load(json_file)\n",
    "    else:\n",
    "        results_dict = {\n",
    "            \"models\": []\n",
    "        }\n",
    "\n",
    "    # Trouver ou ajouter l'entrée pour le modèle spécifié\n",
    "    model_entry = next((model for model in results_dict[\"models\"] if model[\"model\"] == model_name), None)\n",
    "\n",
    "    if not model_entry:\n",
    "        model_entry = {\n",
    "            \"model\": model_name,\n",
    "            \"pollution_metrics\": []\n",
    "        }\n",
    "        results_dict[\"models\"].append(model_entry)\n",
    "\n",
    "    # Mise à jour pour la pollution à 0 (df_clean)\n",
    "    pollution_percentage = 0\n",
    "    existing_entry = next((item for item in model_entry[\"pollution_metrics\"] if item[\"pollution_percentage\"] == pollution_percentage), None)\n",
    "\n",
    "    if existing_entry:\n",
    "        # Remplacer les métriques\n",
    "        existing_entry[\"metrics\"] = {\n",
    "            # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Clean\"]),\n",
    "            \"stability indexes\": 1.0,\n",
    "            \"ARI score\": 1.0,\n",
    "            \"Cluster_Overlap_Size\": 1.0,\n",
    "            \"Cluster_Size_Variance\": 1.0\n",
    "        }\n",
    "    else:\n",
    "        # Ajouter une nouvelle entrée pour ce pourcentage\n",
    "        model_entry[\"pollution_metrics\"].append({\n",
    "            \"pollution_percentage\": pollution_percentage,\n",
    "            \"metrics\": {\n",
    "                # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Clean\"]),\n",
    "                \"stability indexes\": 1.0,\n",
    "                \"ARI score\": 1.0,\n",
    "                \"Cluster_Overlap_Size\": 1.0,\n",
    "                \"Cluster_Size_Variance\": 1.0\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Mise à jour pour les autres niveaux de pollution\n",
    "    for i, pollution_percentage in enumerate(pollution_percentage_levels):\n",
    "        pollution_percentage = int(pollution_percentage*100)\n",
    "        existing_entry = next((item for item in model_entry[\"pollution_metrics\"] if item[\"pollution_percentage\"] == pollution_percentage), None)\n",
    "\n",
    "        if existing_entry:\n",
    "            # Remplacer les métriques\n",
    "            existing_entry[\"metrics\"] = {\n",
    "                # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Noisy\"][i]),\n",
    "                \"stability indexes\": convert_to_float(metrics[\"Stability_Index\"][i]),\n",
    "                \"ARI score\": convert_to_float(metrics[\"ARI\"][i]),\n",
    "                \"Cluster_Overlap_Size\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i]),\n",
    "                \"Cluster_Size_Variance\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i])\n",
    "                \n",
    "            }\n",
    "        else:\n",
    "            # Ajouter une nouvelle entrée pour ce pourcentage\n",
    "            model_entry[\"pollution_metrics\"].append({\n",
    "                \"pollution_percentage\": pollution_percentage,\n",
    "                \"metrics\": {\n",
    "                    # \"silhouette score\": convert_to_float(metrics[\"Silhouette_Score_Noisy\"][i]),\n",
    "                    \"stability indexes\": convert_to_float(metrics[\"Stability_Index\"][i]),\n",
    "                    \"ARI score\": convert_to_float(metrics[\"ARI\"][i]),\n",
    "                    \"Cluster_Overlap_Size\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i]),\n",
    "                    \"Cluster_Size_Variance\": convert_to_float(metrics[\"Cluster_Size_Variance\"][i])\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Écrire les résultats mis à jour dans le fichier JSON\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(results_dict, json_file, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "def prepare_retail_data(df):\n",
    "    # Séparation des caractéristiques numériques et catégorielles\n",
    "    numeric_features = df.select_dtypes(include='number')\n",
    "    categorical_features = df.select_dtypes(include='object')\n",
    "\n",
    "    # Transformation des données numériques\n",
    "    scaler = StandardScaler()\n",
    "    numeric_data_scaled = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Transformation des données catégorielles en utilisant pd.get_dummies\n",
    "    categorical_data_encoded = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "    # Combinaison des données numériques et catégorielles transformées\n",
    "    df_preprocessed = pd.concat([pd.DataFrame(numeric_data_scaled, columns=numeric_features.columns).reset_index(drop=True),\n",
    "                                 categorical_data_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df_preprocessed\n",
    "\n",
    "def compute_metrics(df_clean, df_noisy_dict, clustering_algo=hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5)):\n",
    "    metrics_results = {\n",
    "        \"Stability_Index\": [],\n",
    "        \"ARI\": [],\n",
    "        \"Cluster_Overlap_Size\": [],\n",
    "        \"Cluster_Size_Variance\": []\n",
    "    }\n",
    "\n",
    "    # 1. Réduction de dimensionnalité sur les données propres avec un nouveau modèle UMAP\n",
    "    print(\"Réduction de dimensionnalité sur les données propres\")\n",
    "    umap_model_clean = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "    embedding_clean = umap_model_clean.fit_transform(df_clean)\n",
    "\n",
    "    # 2. Clustering sur les données propres\n",
    "    print(\"Clustering sur les données propres\")\n",
    "    cluster_labels_clean = clustering_algo.fit_predict(embedding_clean)\n",
    "\n",
    "    for key in df_noisy_dict.keys():\n",
    "        df_noisy = df_noisy_dict[key]\n",
    "\n",
    "        if df_noisy is not None:\n",
    "            # Downsample the noisy dataset to match the clean dataset row count\n",
    "            if len(df_noisy) > len(df_clean):\n",
    "                df_noisy_sampled = df_noisy.sample(n=len(df_clean), random_state=42)\n",
    "            else:\n",
    "                df_noisy_sampled = df_noisy\n",
    "\n",
    "            # 3. Réduction de dimensionnalité sur les données bruitées\n",
    "            print(f\"Réduction de dimensionnalité sur les données bruitées: {key}\")\n",
    "            umap_model_noisy = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "            embedding_noisy = umap_model_clean.fit_transform(df_noisy_sampled)\n",
    "\n",
    "            # 4. Clustering sur les données bruitées\n",
    "            print(f\"Clustering sur les données bruitées: {key}\")\n",
    "            cluster_labels_noisy = clustering_algo.fit_predict(embedding_noisy)\n",
    "\n",
    "            # 5. Calcul des métriques\n",
    "\n",
    "            # Adjusted Rand Index (ARI)\n",
    "            print(\"Calcul du Adjusted Rand Index (ARI) sur les données bruitées\")\n",
    "            ari = adjusted_rand_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"ARI\"].append(ari)\n",
    "\n",
    "            # Stability Index (SI)\n",
    "            print(\"Calcul du Stability Index (SI) sur les données bruitées\")\n",
    "            stability_index = normalized_mutual_info_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"Stability_Index\"].append(stability_index)\n",
    "\n",
    "            # Cluster Overlap Size\n",
    "            print(\"Calcul de la taille absolue de l'overlap des clusters\")\n",
    "            overlap_size = compute_cluster_overlap(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"Cluster_Overlap_Size\"].append(overlap_size)\n",
    "\n",
    "            # Cluster Size Variance\n",
    "            print(\"Calcul de la variance de la taille des clusters\")\n",
    "            size_variance = compute_cluster_size_variance(cluster_labels_noisy)\n",
    "            metrics_results[\"Cluster_Size_Variance\"].append(size_variance)\n",
    "        else:\n",
    "            metrics_results[\"ARI\"].append(None)\n",
    "            metrics_results[\"Stability_Index\"].append(None)\n",
    "            metrics_results[\"Cluster_Overlap_Size\"].append(None)\n",
    "            metrics_results[\"Cluster_Size_Variance\"].append(None)\n",
    "\n",
    "    return metrics_results\n",
    "\n",
    "\n",
    "def compute_cluster_overlap(labels1, labels2):\n",
    "    overlap = 0\n",
    "    for cluster1 in np.unique(labels1):\n",
    "        for cluster2 in np.unique(labels2):\n",
    "            intersection = np.sum((labels1 == cluster1) & (labels2 == cluster2))\n",
    "            union = np.sum(labels1 == cluster1) + np.sum(labels2 == cluster2) - intersection\n",
    "            if union > 0:\n",
    "                overlap += intersection / union\n",
    "    return overlap\n",
    "\n",
    "def compute_cluster_size_variance(labels):\n",
    "    sizes = [np.sum(labels == label) for label in np.unique(labels)]\n",
    "    return np.var(sizes) if len(sizes) > 1 else 0\n"
   ],
   "id": "a3c3f83344c0ee25",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Completeness",
   "id": "c4747a0ade73e4d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:08:16.538600Z",
     "start_time": "2024-08-20T18:36:08.320710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilisation de la fonction dans votre code existant\n",
    "missing_percentages = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "# Strategy :\n",
    "strategy_delete = {\n",
    "    'numerical': 'delete',\n",
    "    'categorical': 'delete'\n",
    "}\n",
    "\n",
    "strategy_mean_mode = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "strategy_median_new = {\n",
    "    'numerical': 'median',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_decision_tree = {\n",
    "    'numerical': 'decision_tree',\n",
    "    'categorical': 'decision_tree'\n",
    "}\n",
    "\n",
    "strategy_mean_new = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_knn_mode = {\n",
    "    'numerical': 'knn',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "strategies = [\n",
    "    (\"Mean and Mode\", strategy_mean_mode, \"Mean and Mode\"),\n",
    "    (\"Median and New\", strategy_median_new, \"Median and New\"),\n",
    "    (\"Decision Tree\", strategy_decision_tree, \"Decision Tree\"),\n",
    "    (\"Mean and New\", strategy_mean_new, \"Mean and New\")\n",
    "]\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement du DataSet bruité\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Completeness', 'retail', missing_percentages, strategies)\n",
    "\n",
    "for key, retail_df_strategies in retail_df_noisy_dict.items():\n",
    "    for strategies , retail_df in retail_df_strategies.items():\n",
    "        retail_df_noisy_dict[key][strategies] = prepare_retail_data(retail_df_noisy_dict[key][strategies])\n",
    "        \n",
    "\n",
    "# Chemin du fichier JSON où les résultats seront enregistrés\n",
    "output_path = \"../../Results/Retail/Completeness.json\"\n",
    "\n",
    "# # Calcule des metriques\n",
    "for key, retail_df_strategies in retail_df_noisy_dict.items():\n",
    "    print(key)\n",
    "    metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict[key])\n",
    "    print(metrics)"
   ],
   "id": "39b1d13242670dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20%\n",
      "Réduction de dimensionnalité sur les données propres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.18755664524686638, 0.18749699751118507, 0.7198800809354804, 0.18758791271237601], 'ARI': [0.01708701162787458, 0.017084241740027827, 0.44594683820171444, 0.017082352875360172]}\n",
      "30%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.14780938978842248, 0.14778752467548611, 0.6682536694650523, 0.1477815934259371], 'ARI': [0.011323300069032916, 0.011321705235893922, 0.37172571413075695, 0.011320647121842721]}\n",
      "40%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.1289904198257051, 0.1294750523390299, 0.6525271332447299, 0.12897953669416648], 'ARI': [0.010667321177016275, 0.009722930974323614, 0.41096735917607086, 0.01066663926859575]}\n",
      "50%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.10673462330616901, 0.10684358059032988, 0.558418113849352, 0.10699416052502657], 'ARI': [0.010290609183349996, 0.010373431677227537, 0.379032096398593, 0.010376259433284634]}\n",
      "60%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.07478455858046212, 0.07467300934062944, 0.44008907619316806, 0.07481579792492302], 'ARI': [0.002525451716563507, 0.0025198120152636126, 0.24119091935495413, 0.0025244635834130703]}\n",
      "70%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.07846886535691226, 0.061564547347028736, 0.30021573926127726, 0.05695280058020846], 'ARI': [0.004071434283184314, 0.004473259300756131, 0.17575911662283047, 0.0007897490028301033]}\n",
      "80%\n",
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and Mode\n",
      "Clustering sur les données bruitées: Mean and Mode\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Median and New\n",
      "Clustering sur les données bruitées: Median and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Decision Tree\n",
      "Clustering sur les données bruitées: Decision Tree\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Réduction de dimensionnalité sur les données bruitées: Mean and New\n",
      "Clustering sur les données bruitées: Mean and New\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "{'Stability_Index': [0.09808432177943388, 0.07169860271227806, 0.14142833533387436, 0.07040503452744831], 'ARI': [0.0007555135729522854, 0.0016842114126871487, 0.004537840144664782, 0.0005627170691663838]}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Consistent Representation",
   "id": "6e6a855efa3df705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:28:48.985471Z",
     "start_time": "2024-08-21T13:12:23.645269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Consistent Representation.json\"\n",
    "model_name = \"HDBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Consistent Representation', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "7c7339a93ab068a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: 10%\n",
      "Clustering sur les données bruitées: 10%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 20%\n",
      "Clustering sur les données bruitées: 20%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 30%\n",
      "Clustering sur les données bruitées: 30%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 40%\n",
      "Clustering sur les données bruitées: 40%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 50%\n",
      "Clustering sur les données bruitées: 50%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 60%\n",
      "Clustering sur les données bruitées: 60%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 70%\n",
      "Clustering sur les données bruitées: 70%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 80%\n",
      "Clustering sur les données bruitées: 80%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Results saved to ../../Results/Retail/Consistent Representation.json\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Accuracy",
   "id": "24cc36910ab97073"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T23:33:59.635348Z",
     "start_time": "2024-08-20T23:26:02.662604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Feature Accuracy.json\"\n",
    "model_name = \"HDBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Feature Accuracy', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "cf541bb182da87b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: 10%\n",
      "Clustering sur les données bruitées: 10%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 20%\n",
      "Clustering sur les données bruitées: 20%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 30%\n",
      "Clustering sur les données bruitées: 30%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 40%\n",
      "Clustering sur les données bruitées: 40%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 50%\n",
      "Clustering sur les données bruitées: 50%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 60%\n",
      "Clustering sur les données bruitées: 60%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 70%\n",
      "Clustering sur les données bruitées: 70%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 80%\n",
      "Clustering sur les données bruitées: 80%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Results saved to ../../Results/Retail/Feature Accuracy.json\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Unicity",
   "id": "1016a72967b400dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:45:49.340401Z",
     "start_time": "2024-08-21T13:28:48.986686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../../Results/Retail/Unicity.json\"\n",
    "model_name = \"HDBSCAN\"\n",
    "\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "pollution_percentage_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Unicity', 'retail', pollution_percentage_levels)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)\n",
    "\n",
    "update_json_results(output_path, model_name, metrics, pollution_percentage_levels)"
   ],
   "id": "9c4de03102141fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction de dimensionnalité sur les données propres\n",
      "Clustering sur les données propres\n",
      "Réduction de dimensionnalité sur les données bruitées: 10%\n",
      "Clustering sur les données bruitées: 10%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 20%\n",
      "Clustering sur les données bruitées: 20%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 30%\n",
      "Clustering sur les données bruitées: 30%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 40%\n",
      "Clustering sur les données bruitées: 40%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 50%\n",
      "Clustering sur les données bruitées: 50%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 60%\n",
      "Clustering sur les données bruitées: 60%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 70%\n",
      "Clustering sur les données bruitées: 70%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Réduction de dimensionnalité sur les données bruitées: 80%\n",
      "Clustering sur les données bruitées: 80%\n",
      "Calcul du Adjusted Rand Index (ARI) sur les données bruitées\n",
      "Calcul du Stability Index (SI) sur les données bruitées\n",
      "Calcul de la taille absolue de l'overlap des clusters\n",
      "Calcul de la variance de la taille des clusters\n",
      "Results saved to ../../Results/Retail/Unicity.json\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "4576a273bb3f2724"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:12:32.383690Z",
     "start_time": "2024-08-18T14:12:31.993426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "df"
   ],
   "id": "e585e1c11b84a1c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              City            State  Zipcode    Country   Age  Gender  Income  \\\n",
       "0         Dortmund           Berlin  77985.0    Germany  21.0    Male     Low   \n",
       "1       Nottingham          England  99071.0         UK  19.0  Female     Low   \n",
       "2          Geelong  New South Wales  75929.0  Australia  48.0    Male     Low   \n",
       "3         Edmonton          Ontario  88420.0     Canada  56.0    Male    High   \n",
       "4          Bristol          England  48704.0         UK  22.0    Male     Low   \n",
       "...            ...              ...      ...        ...   ...     ...     ...   \n",
       "293906  Townsville  New South Wales   4567.0  Australia  31.0    Male  Medium   \n",
       "293907     Hanover           Berlin  16852.0    Germany  35.0  Female     Low   \n",
       "293908    Brighton          England  88038.0         UK  41.0    Male     Low   \n",
       "293909     Halifax          Ontario  67608.0     Canada  41.0    Male  Medium   \n",
       "293910      Tucson    West Virginia  25242.0        USA  28.0  Female     Low   \n",
       "\n",
       "       Customer_Segment  Total_Purchases      Amount  ...  Product_Type  \\\n",
       "0               Regular              3.0  108.028757  ...        Shorts   \n",
       "1               Premium              2.0  403.353907  ...        Tablet   \n",
       "2               Regular              3.0  354.477600  ...    Children's   \n",
       "3               Premium              7.0  352.407717  ...         Tools   \n",
       "4               Premium              2.0  124.276524  ...     Chocolate   \n",
       "...                 ...              ...         ...  ...           ...   \n",
       "293906          Regular              5.0  194.792597  ...       Fiction   \n",
       "293907              New              1.0  285.137301  ...        Laptop   \n",
       "293908          Premium              3.0   60.701761  ...        Jacket   \n",
       "293909              New              1.0  120.834784  ...     Furniture   \n",
       "293910          Premium              7.0  340.319059  ...   Decorations   \n",
       "\n",
       "         Feedback Shipping_Method Payment_Method Order_Status Ratings  \\\n",
       "0       Excellent        Same-Day     Debit Card      Shipped     5.0   \n",
       "1       Excellent        Standard    Credit Card   Processing     4.0   \n",
       "2         Average        Same-Day    Credit Card   Processing     2.0   \n",
       "3       Excellent        Standard         PayPal   Processing     4.0   \n",
       "4             Bad        Standard           Cash      Shipped     1.0   \n",
       "...           ...             ...            ...          ...     ...   \n",
       "293906        Bad        Same-Day           Cash   Processing     1.0   \n",
       "293907  Excellent        Same-Day           Cash   Processing     5.0   \n",
       "293908    Average         Express           Cash      Shipped     2.0   \n",
       "293909       Good        Standard           Cash      Shipped     4.0   \n",
       "293910    Average        Same-Day           Cash      Shipped     2.0   \n",
       "\n",
       "                  products Day  Month  Year  \n",
       "0           Cycling shorts  18      9  2023  \n",
       "1               Lenovo Tab  31     12  2023  \n",
       "2         Sports equipment  26      4  2023  \n",
       "3            Utility knife   5      8  2023  \n",
       "4        Chocolate cookies   1     10  2024  \n",
       "...                    ...  ..    ...   ...  \n",
       "293906  Historical fiction  20      1  2024  \n",
       "293907             LG Gram  28     12  2023  \n",
       "293908               Parka  27      2  2024  \n",
       "293909            TV stand   9      3  2023  \n",
       "293910              Clocks   1      8  2024  \n",
       "\n",
       "[293911 rows x 23 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "      <th>Customer_Segment</th>\n",
       "      <th>Total_Purchases</th>\n",
       "      <th>Amount</th>\n",
       "      <th>...</th>\n",
       "      <th>Product_Type</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Shipping_Method</th>\n",
       "      <th>Payment_Method</th>\n",
       "      <th>Order_Status</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>products</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>77985.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Regular</td>\n",
       "      <td>3.0</td>\n",
       "      <td>108.028757</td>\n",
       "      <td>...</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Same-Day</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Cycling shorts</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nottingham</td>\n",
       "      <td>England</td>\n",
       "      <td>99071.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Low</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>403.353907</td>\n",
       "      <td>...</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Processing</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Lenovo Tab</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geelong</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>75929.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>48.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Regular</td>\n",
       "      <td>3.0</td>\n",
       "      <td>354.477600</td>\n",
       "      <td>...</td>\n",
       "      <td>Children's</td>\n",
       "      <td>Average</td>\n",
       "      <td>Same-Day</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Processing</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Sports equipment</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edmonton</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>88420.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>High</td>\n",
       "      <td>Premium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>352.407717</td>\n",
       "      <td>...</td>\n",
       "      <td>Tools</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Standard</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Processing</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Utility knife</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bristol</td>\n",
       "      <td>England</td>\n",
       "      <td>48704.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.276524</td>\n",
       "      <td>...</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chocolate cookies</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293906</th>\n",
       "      <td>Townsville</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>4567.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Regular</td>\n",
       "      <td>5.0</td>\n",
       "      <td>194.792597</td>\n",
       "      <td>...</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Same-Day</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Processing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Historical fiction</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293907</th>\n",
       "      <td>Hanover</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>16852.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Low</td>\n",
       "      <td>New</td>\n",
       "      <td>1.0</td>\n",
       "      <td>285.137301</td>\n",
       "      <td>...</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Same-Day</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Processing</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LG Gram</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293908</th>\n",
       "      <td>Brighton</td>\n",
       "      <td>England</td>\n",
       "      <td>88038.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Low</td>\n",
       "      <td>Premium</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.701761</td>\n",
       "      <td>...</td>\n",
       "      <td>Jacket</td>\n",
       "      <td>Average</td>\n",
       "      <td>Express</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Parka</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293909</th>\n",
       "      <td>Halifax</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>67608.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.834784</td>\n",
       "      <td>...</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Good</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>4.0</td>\n",
       "      <td>TV stand</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293910</th>\n",
       "      <td>Tucson</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>25242.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Low</td>\n",
       "      <td>Premium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>340.319059</td>\n",
       "      <td>...</td>\n",
       "      <td>Decorations</td>\n",
       "      <td>Average</td>\n",
       "      <td>Same-Day</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Clocks</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293911 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:12:32.637794Z",
     "start_time": "2024-08-18T14:12:32.384398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Séparation des caractéristiques numériques et catégorielles\n",
    "numeric_features = df.select_dtypes(include='number')\n",
    "categorical_features = df.select_dtypes(include='object')\n",
    "\n",
    "# Transformation des données numériques\n",
    "scaler = StandardScaler()\n",
    "numeric_data_scaled = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Transformation des données catégorielles en utilisant pd.get_dummies\n",
    "categorical_data_encoded = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "# Combinaison des données numériques et catégorielles transformées\n",
    "df_preprocessed = pd.concat([pd.DataFrame(numeric_data_scaled, columns=numeric_features.columns).reset_index(drop=True),\n",
    "                             categorical_data_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Affichage des premières lignes du DataFrame prétraité\n",
    "df_preprocessed"
   ],
   "id": "1451893131fd17f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Zipcode       Age  Total_Purchases    Amount  Total_Amount   Ratings  \\\n",
       "0       0.955828 -0.963246        -0.822701 -1.040570     -0.924446  1.391396   \n",
       "1       1.683520 -1.096422        -1.171323  1.048181     -0.496928  0.634256   \n",
       "2       0.884874  0.834629        -0.822701  0.702493     -0.269515 -0.880025   \n",
       "3       1.315947  1.367333         0.571788  0.687853      0.973668  0.634256   \n",
       "4      -0.054678 -0.896658        -1.171323 -0.925654     -0.991355 -1.637165   \n",
       "...          ...       ...              ...       ...           ...       ...   \n",
       "293906 -1.577875 -0.297366        -0.125457 -0.426914     -0.348770 -1.637165   \n",
       "293907 -1.153911 -0.031014        -1.519945  0.212068     -0.958948  1.391396   \n",
       "293908  1.302764  0.368513        -0.822701 -1.375301     -1.050216 -0.880025   \n",
       "293909  0.597711  0.368513        -1.519945 -0.949997     -1.104491  0.634256   \n",
       "293910 -0.864367 -0.497130         0.571788  0.602353      0.898709 -0.880025   \n",
       "\n",
       "             Day     Month      Year  City_Albuquerque  ...  \\\n",
       "0       0.254179  0.721840 -0.444729             False  ...   \n",
       "1       1.728691  1.590416 -0.444729             False  ...   \n",
       "2       1.161571 -0.725787 -0.444729             False  ...   \n",
       "3      -1.220333  0.432314 -0.444729             False  ...   \n",
       "4      -1.674029  1.011365  2.248561             False  ...   \n",
       "...          ...       ...       ...               ...  ...   \n",
       "293906  0.481027 -1.594363  2.248561             False  ...   \n",
       "293907  1.388419  1.590416 -0.444729             False  ...   \n",
       "293908  1.274995 -1.304838  2.248561             False  ...   \n",
       "293909 -0.766637 -1.015312 -0.444729             False  ...   \n",
       "293910 -1.674029  0.432314  2.248561             False  ...   \n",
       "\n",
       "        products_White chocolate  products_Wide-leg jeans  \\\n",
       "0                          False                    False   \n",
       "1                          False                    False   \n",
       "2                          False                    False   \n",
       "3                          False                    False   \n",
       "4                          False                    False   \n",
       "...                          ...                      ...   \n",
       "293906                     False                    False   \n",
       "293907                     False                    False   \n",
       "293908                     False                    False   \n",
       "293909                     False                    False   \n",
       "293910                     False                    False   \n",
       "\n",
       "        products_Windbreaker  products_Window AC  \\\n",
       "0                      False               False   \n",
       "1                      False               False   \n",
       "2                      False               False   \n",
       "3                      False               False   \n",
       "4                      False               False   \n",
       "...                      ...                 ...   \n",
       "293906                 False               False   \n",
       "293907                 False               False   \n",
       "293908                 False               False   \n",
       "293909                 False               False   \n",
       "293910                 False               False   \n",
       "\n",
       "        products_Wireless headphones  products_Wrap dress  products_Wrench  \\\n",
       "0                              False                False            False   \n",
       "1                              False                False            False   \n",
       "2                              False                False            False   \n",
       "3                              False                False            False   \n",
       "4                              False                False            False   \n",
       "...                              ...                  ...              ...   \n",
       "293906                         False                False            False   \n",
       "293907                         False                False            False   \n",
       "293908                         False                False            False   \n",
       "293909                         False                False            False   \n",
       "293910                         False                False            False   \n",
       "\n",
       "        products_Xiaomi Mi  products_iPad  products_iPhone  \n",
       "0                    False          False            False  \n",
       "1                    False          False            False  \n",
       "2                    False          False            False  \n",
       "3                    False          False            False  \n",
       "4                    False          False            False  \n",
       "...                    ...            ...              ...  \n",
       "293906               False          False            False  \n",
       "293907               False          False            False  \n",
       "293908               False          False            False  \n",
       "293909               False          False            False  \n",
       "293910               False          False            False  \n",
       "\n",
       "[293911 rows x 581 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Purchases</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Total_Amount</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>City_Albuquerque</th>\n",
       "      <th>...</th>\n",
       "      <th>products_White chocolate</th>\n",
       "      <th>products_Wide-leg jeans</th>\n",
       "      <th>products_Windbreaker</th>\n",
       "      <th>products_Window AC</th>\n",
       "      <th>products_Wireless headphones</th>\n",
       "      <th>products_Wrap dress</th>\n",
       "      <th>products_Wrench</th>\n",
       "      <th>products_Xiaomi Mi</th>\n",
       "      <th>products_iPad</th>\n",
       "      <th>products_iPhone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955828</td>\n",
       "      <td>-0.963246</td>\n",
       "      <td>-0.822701</td>\n",
       "      <td>-1.040570</td>\n",
       "      <td>-0.924446</td>\n",
       "      <td>1.391396</td>\n",
       "      <td>0.254179</td>\n",
       "      <td>0.721840</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.683520</td>\n",
       "      <td>-1.096422</td>\n",
       "      <td>-1.171323</td>\n",
       "      <td>1.048181</td>\n",
       "      <td>-0.496928</td>\n",
       "      <td>0.634256</td>\n",
       "      <td>1.728691</td>\n",
       "      <td>1.590416</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.884874</td>\n",
       "      <td>0.834629</td>\n",
       "      <td>-0.822701</td>\n",
       "      <td>0.702493</td>\n",
       "      <td>-0.269515</td>\n",
       "      <td>-0.880025</td>\n",
       "      <td>1.161571</td>\n",
       "      <td>-0.725787</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.315947</td>\n",
       "      <td>1.367333</td>\n",
       "      <td>0.571788</td>\n",
       "      <td>0.687853</td>\n",
       "      <td>0.973668</td>\n",
       "      <td>0.634256</td>\n",
       "      <td>-1.220333</td>\n",
       "      <td>0.432314</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.054678</td>\n",
       "      <td>-0.896658</td>\n",
       "      <td>-1.171323</td>\n",
       "      <td>-0.925654</td>\n",
       "      <td>-0.991355</td>\n",
       "      <td>-1.637165</td>\n",
       "      <td>-1.674029</td>\n",
       "      <td>1.011365</td>\n",
       "      <td>2.248561</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293906</th>\n",
       "      <td>-1.577875</td>\n",
       "      <td>-0.297366</td>\n",
       "      <td>-0.125457</td>\n",
       "      <td>-0.426914</td>\n",
       "      <td>-0.348770</td>\n",
       "      <td>-1.637165</td>\n",
       "      <td>0.481027</td>\n",
       "      <td>-1.594363</td>\n",
       "      <td>2.248561</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293907</th>\n",
       "      <td>-1.153911</td>\n",
       "      <td>-0.031014</td>\n",
       "      <td>-1.519945</td>\n",
       "      <td>0.212068</td>\n",
       "      <td>-0.958948</td>\n",
       "      <td>1.391396</td>\n",
       "      <td>1.388419</td>\n",
       "      <td>1.590416</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293908</th>\n",
       "      <td>1.302764</td>\n",
       "      <td>0.368513</td>\n",
       "      <td>-0.822701</td>\n",
       "      <td>-1.375301</td>\n",
       "      <td>-1.050216</td>\n",
       "      <td>-0.880025</td>\n",
       "      <td>1.274995</td>\n",
       "      <td>-1.304838</td>\n",
       "      <td>2.248561</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293909</th>\n",
       "      <td>0.597711</td>\n",
       "      <td>0.368513</td>\n",
       "      <td>-1.519945</td>\n",
       "      <td>-0.949997</td>\n",
       "      <td>-1.104491</td>\n",
       "      <td>0.634256</td>\n",
       "      <td>-0.766637</td>\n",
       "      <td>-1.015312</td>\n",
       "      <td>-0.444729</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293910</th>\n",
       "      <td>-0.864367</td>\n",
       "      <td>-0.497130</td>\n",
       "      <td>0.571788</td>\n",
       "      <td>0.602353</td>\n",
       "      <td>0.898709</td>\n",
       "      <td>-0.880025</td>\n",
       "      <td>-1.674029</td>\n",
       "      <td>0.432314</td>\n",
       "      <td>2.248561</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293911 rows × 581 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:13:27.060588Z",
     "start_time": "2024-08-18T14:12:34.749069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Réduction de la dimensionalité avec UMAP\n",
    "umap = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, n_jobs=-1)  # Utilisation de tous les cœurs disponibles\n",
    "embedding = umap.fit_transform(df_preprocessed)"
   ],
   "id": "6ef8a279e5cab8b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:14:53.786054Z",
     "start_time": "2024-08-18T14:14:50.220023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Application du clustering HDBSCAN\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean')\n",
    "cluster_labels = hdbscan_model.fit_predict(embedding)"
   ],
   "id": "96e0bfa158cf1c69",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:16:03.501500Z",
     "start_time": "2024-08-18T14:16:03.482545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Nombre de clusters trouvés (en excluant le bruit)\n",
    "num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "print(f\"Nombre de clusters trouvés: {num_clusters}\")\n",
    "print(f\"Distribution des clusters: {np.unique(cluster_labels, return_counts=True)}\")"
   ],
   "id": "e02e06939b05d28f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de clusters trouvés: 258\n",
      "Distribution des clusters: (array([ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
      "        12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,\n",
      "        25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
      "        38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,\n",
      "        51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
      "        64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
      "        77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
      "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
      "       103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
      "       116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
      "       129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "       142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
      "       155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,\n",
      "       181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
      "       194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "       207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "       220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
      "       233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245,\n",
      "       246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257]), array([ 8423,  1145,  9580,  1075,  1100,  2792,   738,  9699,  9690,\n",
      "        4997,  7963,  9518,    11,  1738,   837,  7962,  6626, 19912,\n",
      "        6852,  7987, 19951,   739,  7937, 19973,  2169,  5203,   425,\n",
      "         688,   376,    13,  6887,  1209, 20150,  6609,  6845,  6500,\n",
      "        5315, 10457,    10,  5791,   449, 18672,  3078,  6416,  1264,\n",
      "          10,   517,    15,    16,    16,  2451,    15,    12,    15,\n",
      "          16,    14,    23,    11,    12,    11,    10,    11,    11,\n",
      "          11,  8719,    19,    17,    14,    10,    25,    13,    13,\n",
      "          23,    10,    17,    12,    10,    16,    10,    12,    19,\n",
      "          13,    12,    14,    12,    14,    33,    25,    20,    18,\n",
      "          11,    26,    11,    13,    15,    11,    17,    21,    38,\n",
      "          11,    12,    27,    10,    11,    11,    35,   402,    13,\n",
      "          10,    16,    19,    26,    22,    18,    32,    16,    44,\n",
      "          88,    10,    22,    17,    35,    45,    22,    18,    26,\n",
      "          23,    21,    37,    15,    24,    18,    13,    16,    10,\n",
      "          23,    53,    13,    15,    11,   397,    14,    16,    20,\n",
      "          11,    14,    13,    14,    23,    10,    18,    10,    31,\n",
      "          14,    15,    43,    14,    11,    37,    11,    54,    15,\n",
      "          12,    14,    20,    25,    10,    25,    12,    13,    25,\n",
      "          23,    12,    13,    39,    10,    12,    50,    14,    71,\n",
      "          26,    57,    17,    67,    43,    39,    15,    18,    19,\n",
      "         363,    10,    20,    17,    30,    26,    30,    10,    22,\n",
      "          23,    21,    15,    17,    11,    45,    11,    23,    34,\n",
      "          21,    14,    24,    14,    60,    32,    31,    20,    18,\n",
      "          11,    21,    19,    11,    56,    23,    18,    19,    25,\n",
      "          23,    24,    60,    36,    41,    36,    21,    17,    15,\n",
      "          25,    19,    17,    19,    23,    12,    24,    47,    22,\n",
      "          58,    29,    18,    61,   840,    13,    16,    35,    13,\n",
      "          11,    16,    22,    15,    10,    13,    28]))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T14:23:41.619307Z",
     "start_time": "2024-08-18T14:16:15.707979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Évaluation du Clustering\n",
    "\n",
    "# AMI - Adjusted Mutual Information\n",
    "# Vous devez avoir les étiquettes vraies (y_true) pour cette évaluation\n",
    "# AMI est généralement utilisé lorsque vous avez des étiquettes de référence\n",
    "# ami_score = adjusted_mutual_info_score(y_true, cluster_labels)\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_avg = silhouette_score(embedding, cluster_labels)\n",
    "\n",
    "# Variance des tailles des clusters\n",
    "cluster_sizes = np.array([count for label, count in Counter(cluster_labels).items() if label != -1])  # Ignorer les outliers\n",
    "variance_cluster_size = np.var(cluster_sizes)\n",
    "\n",
    "# Calcul de l'Overlap (en utilisant une approximation via la distance intra-cluster)\n",
    "def calculate_overlap(labels, data):\n",
    "    cluster_centers = []\n",
    "    for label in np.unique(labels):\n",
    "        if label != -1:  # Ignorer les outliers\n",
    "            cluster_data = data[labels == label]\n",
    "            center = np.mean(cluster_data, axis=0)\n",
    "            cluster_centers.append(center)\n",
    "\n",
    "    # Distance entre chaque cluster center\n",
    "    distances = cdist(cluster_centers, cluster_centers, 'euclidean')\n",
    "    np.fill_diagonal(distances, np.inf)\n",
    "    overlap = np.sum(distances < np.mean(distances))\n",
    "\n",
    "    return overlap\n",
    "\n",
    "overlap_score = calculate_overlap(cluster_labels, embedding)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n",
    "print(f'Variance des tailles de clusters: {variance_cluster_size}')\n",
    "print(f'Score Overlap: {overlap_score}')\n",
    "# print(f'AMI: {ami_score}')  # Activer ceci si vous avez y_true"
   ],
   "id": "b338af556dfdf7e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.39225906133651733\n",
      "Variance des tailles de clusters: 11277709.093143443\n",
      "Score Overlap: 66306\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:25:20.060797Z",
     "start_time": "2024-08-19T08:21:51.922960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_mutual_info_score, silhouette_score, adjusted_rand_score, \\\n",
    "    normalized_mutual_info_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from collections import Counter\n",
    "\n",
    "def prepare_retail_data(df):\n",
    "    # Séparation des caractéristiques numériques et catégorielles\n",
    "    numeric_features = df.select_dtypes(include='number')\n",
    "    categorical_features = df.select_dtypes(include='object')\n",
    "\n",
    "    # Transformation des données numériques\n",
    "    scaler = StandardScaler()\n",
    "    numeric_data_scaled = scaler.fit_transform(numeric_features)\n",
    "\n",
    "    # Transformation des données catégorielles en utilisant pd.get_dummies\n",
    "    categorical_data_encoded = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "    # Combinaison des données numériques et catégorielles transformées\n",
    "    df_preprocessed = pd.concat([pd.DataFrame(numeric_data_scaled, columns=numeric_features.columns).reset_index(drop=True),\n",
    "                                 categorical_data_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df_preprocessed\n",
    "\n",
    "def compute_metrics(df_clean, df_noisy_dict, clustering_algo=hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5)):\n",
    "    metrics_results = {\n",
    "        \"Stability_Index\": [],\n",
    "        \"ARI\": [],\n",
    "        \"Silhouette_Score_Clean\": None,\n",
    "        \"Silhouette_Score_Noisy\": []\n",
    "    }\n",
    "\n",
    "    # 1. Réduction de dimensionnalité sur les données propres\n",
    "    print(\"Réduction de dimensionnalité\")\n",
    "    umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\n",
    "    embedding_clean = umap_model.fit_transform(df_clean)\n",
    "    # 2. Clustering sur les données propres\n",
    "    print(\"Clustering\")\n",
    "    cluster_labels_clean = clustering_algo.fit_predict(embedding_clean)\n",
    "\n",
    "    # Calcul du Silhouette Score sur les données propres\n",
    "    print(\"Calcul du Silhouette Score\")\n",
    "    silhouette_clean = silhouette_score(embedding_clean, cluster_labels_clean)\n",
    "    metrics_results[\"Silhouette_Score_Clean\"] = silhouette_clean\n",
    "\n",
    "    for key in df_noisy_dict.keys():\n",
    "        print(key)\n",
    "        df_noisy = df_noisy_dict[key]\n",
    "\n",
    "        if df_noisy is not None:\n",
    "            # 3. Réduction de dimensionnalité sur les données bruitées\n",
    "            embedding_noisy = umap_model.transform(df_noisy)\n",
    "\n",
    "            # 4. Clustering sur les données bruitées\n",
    "            cluster_labels_noisy = clustering_algo.fit_predict(embedding_noisy)\n",
    "\n",
    "            # 5. Calcul des métriques\n",
    "\n",
    "            # Adjusted Rand Index (ARI)\n",
    "            ari = adjusted_rand_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"ARI\"].append(ari)\n",
    "\n",
    "            # Silhouette Score sur les données bruitées\n",
    "            silhouette_noisy = silhouette_score(embedding_noisy, cluster_labels_noisy)\n",
    "            metrics_results[\"Silhouette_Score_Noisy\"].append(silhouette_noisy)\n",
    "\n",
    "            # Stability Index (SI) - en utilisant l'AMI comme proxy\n",
    "            stability_index = normalized_mutual_info_score(cluster_labels_clean, cluster_labels_noisy)\n",
    "            metrics_results[\"Stability_Index\"].append(stability_index)\n",
    "        else:\n",
    "            # Si le dataframe n'est pas disponible, on ajoute None pour ce niveau de bruit\n",
    "            metrics_results[\"ARI\"].append(None)\n",
    "            metrics_results[\"Silhouette_Score_Noisy\"].append(None)\n",
    "            metrics_results[\"Stability_Index\"].append(None)\n",
    "\n",
    "    return metrics_results\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Chargement du DataSet clean\n",
    "retail_df_clean = pd.read_csv('../../Data/Clustering/retail Data/retail_data_clean.csv')\n",
    "retail_df_clean = prepare_retail_data(retail_df_clean)\n",
    "\n",
    "# Chargement des DataSet pollués\n",
    "missing_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "retail_df_noisy_dict = read_csv_files_to_dict('../../Data/Clustering/retail Data', 'Feature Accuracy', 'retail', missing_percentages)\n",
    "\n",
    "for key in retail_df_noisy_dict.keys():\n",
    "    if retail_df_noisy_dict[key] is not None:\n",
    "        retail_df_noisy_dict[key] = prepare_retail_data(retail_df_noisy_dict[key])\n",
    "\n",
    "metrics = compute_metrics(retail_df_clean, retail_df_noisy_dict)"
   ],
   "id": "da83864d33568e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:13:51.075665Z",
     "start_time": "2024-08-19T13:13:51.071286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Affichage des résultats\n",
    "metrics"
   ],
   "id": "a09e87126ccb219d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Stability_Index': [0.6827248633820063,\n",
       "  0.43575938626099814,\n",
       "  0.1721664525072392,\n",
       "  0.16053127765393882,\n",
       "  0.14671187464421312,\n",
       "  0.13618995316396276,\n",
       "  0.125958444437765,\n",
       "  0.12020514270552757],\n",
       " 'ARI': [0.43532076566190603,\n",
       "  0.12633221200716088,\n",
       "  0.031490041728447084,\n",
       "  0.03005326058548987,\n",
       "  0.027562095145463674,\n",
       "  0.025858072089500632,\n",
       "  0.023978171084148653,\n",
       "  0.042972166286943655],\n",
       " 'Silhouette_Score_Clean': 0.4796416,\n",
       " 'Silhouette_Score_Noisy': [-0.027544223,\n",
       "  -0.07346046,\n",
       "  0.37912834,\n",
       "  0.55803615,\n",
       "  0.5646407,\n",
       "  0.40605074,\n",
       "  0.5764017,\n",
       "  -0.07651526]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7a3d206e22e1cf0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
