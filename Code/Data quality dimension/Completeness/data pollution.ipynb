{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-21T07:03:07.916892Z",
     "start_time": "2024-08-21T07:03:07.900099Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T07:03:08.486836Z",
     "start_time": "2024-08-21T07:03:08.459341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def impute_numerical_by_delete(df):\n",
    "    df_cleaned = df.copy()\n",
    "    numerical_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "    df_cleaned = df_cleaned.dropna(subset=numerical_cols)\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def impute_numerical_by_mean(df):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed[df_imputed.select_dtypes(include='number').columns] = imputer.fit_transform(\n",
    "        df_imputed.select_dtypes(include='number'))\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_numerical_by_median(df):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_imputed[df_imputed.select_dtypes(include='number').columns] = imputer.fit_transform(\n",
    "        df_imputed.select_dtypes(include='number'))\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_numerical_by_knn(df):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_imputed[df_imputed.select_dtypes(include='number').columns] = imputer.fit_transform(\n",
    "        df_imputed.select_dtypes(include='number'))\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_numerical_by_multiple(df):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = IterativeImputer()\n",
    "    df_imputed[df_imputed.select_dtypes(include='number').columns] = imputer.fit_transform(\n",
    "        df_imputed.select_dtypes(include='number'))\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_numerical_by_decision_tree(df):\n",
    "    df_imputed = df.copy()\n",
    "    numerical_columns = df_imputed.select_dtypes(include='number').columns\n",
    "    categorical_columns = df_imputed.select_dtypes(include='object').columns\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        if df_imputed[col].isnull().sum() > 0:\n",
    "            # Separate known and unknown data\n",
    "            known_data = df_imputed[df_imputed[col].notna()]\n",
    "            unknown_data = df_imputed[df_imputed[col].isna()]\n",
    "\n",
    "            if not unknown_data.empty:\n",
    "                # Convert categorical columns to dummies\n",
    "                known_data_dummies = pd.get_dummies(known_data, columns=categorical_columns, drop_first=False)\n",
    "                unknown_data_dummies = pd.get_dummies(unknown_data, columns=categorical_columns, drop_first=False)\n",
    "\n",
    "                # Align the columns to ensure they match\n",
    "                known_data_dummies, unknown_data_dummies = known_data_dummies.align(unknown_data_dummies, join='left', axis=1, fill_value=0)\n",
    "\n",
    "                # Train the model on known data\n",
    "                model = DecisionTreeRegressor()\n",
    "                model.fit(known_data_dummies.drop(columns=[col]), known_data[col])\n",
    "\n",
    "                # Predict missing values and replace them\n",
    "                df_imputed.loc[df_imputed[col].isna(), col] = model.predict(unknown_data_dummies.drop(columns=[col]))\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# Imputation pour les données catégoriques\n",
    "def impute_categorical_by_delete(df):\n",
    "    df_cleaned = df.copy()\n",
    "    categorical_cols = df_cleaned.select_dtypes(include='object').columns\n",
    "    df_cleaned = df_cleaned.dropna(subset=categorical_cols)\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def impute_categorical_by_mode(df):\n",
    "    df_imputed = df.copy()\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df_imputed[df_imputed.select_dtypes(include='object').columns] = imputer.fit_transform(\n",
    "        df_imputed.select_dtypes(include='object'))\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_categorical_by_new_category(df):\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed = df_imputed.fillna('Missing')\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_categorical_by_hot_deck(df):\n",
    "    df_imputed = df.copy()\n",
    "    for col in df_imputed.columns:\n",
    "        missing_indices = df_imputed[col].isna()\n",
    "        if missing_indices.any():\n",
    "            available_values = df_imputed[col].dropna().values\n",
    "            imputed_values = np.random.choice(available_values, size=missing_indices.sum())\n",
    "            df_imputed.loc[missing_indices, col] = imputed_values\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def impute_categorical_by_decision_tree(df):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    object_columns = df_imputed.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in object_columns:\n",
    "        if df_imputed[col].isnull().sum() > 0:\n",
    "            # Split the DataFrame into two sets: with and without missing values for the current column\n",
    "            df_missing = df_imputed[df_imputed[col].isnull()]\n",
    "            df_not_missing = df_imputed[~df_imputed[col].isnull()]\n",
    "\n",
    "            # If all values are missing, we cannot predict anything\n",
    "            if df_not_missing.empty:\n",
    "                continue\n",
    "\n",
    "            # Define features (X) and target (y)\n",
    "            X = df_not_missing.drop(columns=[col])\n",
    "            y = df_not_missing[col].astype(str)  # Ensure the target is categorical\n",
    "\n",
    "            # Encode the remaining object columns\n",
    "            X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "            # Encode the missing set and align columns\n",
    "            X_missing = pd.get_dummies(df_missing.drop(columns=[col]), drop_first=True)\n",
    "\n",
    "            # Align columns to ensure consistency\n",
    "            X, X_missing = X.align(X_missing, join='left', axis=1, fill_value=0)\n",
    "\n",
    "            # Ensure there are no duplicated columns\n",
    "            X = X.loc[:, ~X.columns.duplicated()]\n",
    "            X_missing = X_missing.loc[:, ~X_missing.columns.duplicated()]\n",
    "\n",
    "            # Create and train the model\n",
    "            model = DecisionTreeClassifier(random_state=42)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Predict the missing values\n",
    "            df_imputed.loc[df_imputed[col].isnull(), col] = model.predict(X_missing)\n",
    "\n",
    "    return df_imputed"
   ],
   "id": "16f001b8a3164c30",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CHARGEMENTS DES DONNEES",
   "id": "82dd3577936034b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:38:31.973573Z",
     "start_time": "2024-08-17T22:38:30.458600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chargement des DataSets\n",
    "fps_df = pd.read_csv('../../../Data/Regression/fps in video games/fps-in-video-games_clean.csv', )\n",
    "heart_df = pd.read_csv('../../../Data/Classification/Indicators of Heart Disease/heart_2022_no_nans_clean.csv')\n",
    "retail_df = pd.read_csv('../../../Data/Clustering/retail Data/retail_data_clean.csv')"
   ],
   "id": "c621265dc0224f5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/m20c7_k95j52q7xcg6lwyd7w0000gn/T/ipykernel_16808/3500090452.py:2: DtypeWarning: Columns (25,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fps_df = pd.read_csv('../../../Data/Regression/fps in video games/fps-in-video-games_clean.csv')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SUPPRESSION DES DONNEES",
   "id": "3a92d7c7c1026c36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:38:35.462868Z",
     "start_time": "2024-08-17T22:38:35.458861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def introduce_missing_values_mcar(df, missing_percentages, target=None):\n",
    "    # Vérifie que les pourcentages sont valides\n",
    "    if not all(0 <= p <= 1 for p in missing_percentages):\n",
    "        raise ValueError(\"Les pourcentages doivent être entre 0 et 1.\")\n",
    "\n",
    "    # Si une target est spécifiée, vérifie que la colonne existe dans le DataFrame\n",
    "    if target and target not in df.columns:\n",
    "        raise ValueError(f\"La colonne target '{target}' n'existe pas dans le DataFrame.\")\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for percentage in missing_percentages:\n",
    "        df_missing = df.copy()\n",
    "        # Pour chaque colonne du DataFrame\n",
    "        for col in df.columns:\n",
    "            # Si une target est spécifiée, on exclut cette colonne\n",
    "            if target and col == target:\n",
    "                continue  # On saute la colonne target\n",
    "            # Calcule le nombre de valeurs à remplacer par NaN\n",
    "            n_missing = int(np.floor(percentage * df.shape[0]))\n",
    "            # Sélectionne aléatoirement les index des valeurs à remplacer\n",
    "            missing_indices = np.random.choice(df.index, n_missing, replace=False)\n",
    "            # Remplace les valeurs par NaN\n",
    "            df_missing.loc[missing_indices, col] = np.nan\n",
    "\n",
    "        df_dict[f\"{int(percentage * 100)}%\"] = df_missing\n",
    "\n",
    "    return df_dict"
   ],
   "id": "725440d5fef5f958",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:38:43.740596Z",
     "start_time": "2024-08-17T22:38:37.737441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Liste des pourcentages de valeurs manquantes à introduire\n",
    "missing_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Génération des DataFrames avec valeurs manquantes\n",
    "fps_df_with_missing = introduce_missing_values_mcar(fps_df, missing_percentages, 'FPS')\n",
    "heart_df_with_missing = introduce_missing_values_mcar(heart_df, missing_percentages, 'HadHeartAttack')\n",
    "retail_df_with_missing = introduce_missing_values_mcar(retail_df, missing_percentages)"
   ],
   "id": "f34db870e6774e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:39:33.367378Z",
     "start_time": "2024-08-17T22:38:46.013251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enregistrement des DataFrames générés\n",
    "for key, value in fps_df_with_missing.items():\n",
    "    value.to_csv(f\"../../../Data/Regression/fps in video games/Completeness/NaN/fps_{key}.csv\", index=False)\n",
    "\n",
    "for key, value in heart_df_with_missing.items():\n",
    "    value.to_csv(f\"../../../Data/Classification/Indicators of Heart Disease/Completeness/NaN/heart_{key}.csv\", index=False)\n",
    "\n",
    "for key, value in retail_df_with_missing.items():\n",
    "    value.to_csv(f\"../../../Data/Clustering/retail Data/Completeness/NaN/retail_{key}.csv\", index=False)"
   ],
   "id": "369eee28d4a1070",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correction des données avec differentes Strategies",
   "id": "ee598ad14739a92a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T07:03:18.782137Z",
     "start_time": "2024-08-21T07:03:18.768699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def impute_dataset(df, strategy):\n",
    "    df_imputed = df.copy()\n",
    "    # Impute numerical columns\n",
    "    if strategy['numerical'] == 'delete':\n",
    "        df_imputed = impute_numerical_by_delete(df_imputed)\n",
    "    elif strategy['numerical'] == 'mean':\n",
    "        df_imputed = impute_numerical_by_mean(df_imputed)\n",
    "    elif strategy['numerical'] == 'median':\n",
    "        df_imputed = impute_numerical_by_median(df_imputed)\n",
    "    elif strategy['numerical'] == 'knn':\n",
    "        df_imputed = impute_numerical_by_knn(df_imputed)\n",
    "    elif strategy['numerical'] == 'multiple':\n",
    "        df_imputed = impute_numerical_by_multiple(df_imputed)\n",
    "    elif strategy['numerical'] == 'decision_tree':\n",
    "        df_imputed = impute_numerical_by_decision_tree(df_imputed)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown numerical imputation strategy\")\n",
    "\n",
    "    # Impute categorical columns\n",
    "    if strategy['categorical'] == 'delete':\n",
    "        df_imputed = impute_categorical_by_delete(df_imputed)\n",
    "    elif strategy['categorical'] == 'mode':\n",
    "        df_imputed = impute_categorical_by_mode(df_imputed)\n",
    "    elif strategy['categorical'] == 'new':\n",
    "        df_imputed = impute_categorical_by_new_category(df_imputed)\n",
    "    elif strategy['categorical'] == 'hot_deck':\n",
    "        df_imputed = impute_categorical_by_hot_deck(df_imputed)\n",
    "    elif strategy['categorical'] == 'decision_tree':\n",
    "        df_imputed = impute_categorical_by_decision_tree(df_imputed)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown categorical imputation strategy\")\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "# Strategy :\n",
    "strategy_delete = {\n",
    "    'numerical': 'delete',\n",
    "    'categorical': 'delete'\n",
    "}\n",
    "\n",
    "strategy_mean_mode = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "strategy_median_new = {\n",
    "    'numerical': 'median',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_decision_tree = {\n",
    "    'numerical': 'decision_tree',\n",
    "    'categorical': 'decision_tree'\n",
    "}\n",
    "\n",
    "strategy_mean_new = {\n",
    "    'numerical': 'mean',\n",
    "    'categorical': 'new'\n",
    "}\n",
    "\n",
    "strategy_knn_mode = {\n",
    "    'numerical': 'knn',\n",
    "    'categorical': 'mode'\n",
    "}"
   ],
   "id": "3573d47c4f96b06b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:07:10.500860Z",
     "start_time": "2024-08-21T11:08:04.731768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_files(base_path, file_prefix, missing_percentages, strategies):\n",
    "    for percentage in missing_percentages:\n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(f\"{base_path}/NaN/{file_prefix}_{int(percentage * 100)}%.csv\")\n",
    "        print(percentage)\n",
    "        # Appliquer chaque stratégie d'imputation et sauvegarder les résultats\n",
    "        for strategy_name, strategy_func, subfolder in strategies:\n",
    "            df_imputed = impute_dataset(df, strategy_func)\n",
    "            df_imputed.to_csv(f\"{base_path}/{subfolder}/{file_prefix}_{int(percentage * 100)}%.csv\")\n",
    "\n",
    "# Les stratégies d'imputation, définies sous forme de tuples avec le nom de la stratégie, la fonction correspondante et le sous-dossier où enregistrer le fichier\n",
    "strategies = [\n",
    "    # (\"Delete lines\", strategy_delete, \"Delete lines\"),\n",
    "    # (\"Mean and Mode\", strategy_mean_mode, \"Mean and Mode\"),\n",
    "    # (\"Median and New\", strategy_median_new, \"Median and New\"),\n",
    "    # (\"Decision Tree\", strategy_decision_tree, \"Decision Tree\"),\n",
    "    # (\"Mean and New\", strategy_mean_new, \"Mean and New\")\n",
    "    (\"KNN and Mode\", strategy_knn_mode, \"KNN and Mode\")\n",
    "]\n",
    "\n",
    "missing_percentages = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Chemins de base pour les différents ensembles de données\n",
    "heart_base_path = \"../../../Data/Classification/Indicators of Heart Disease/Completeness\"\n",
    "retail_base_path = \"../../../Data/Clustering/retail Data/Completeness\"\n",
    "fps_base_path = \"../../../Data/Regression/fps in video games/Completeness\"\n",
    "\n",
    "# Appeler la fonction pour chaque ensemble de données\n",
    "# process_files(heart_base_path, \"heart\", missing_percentages, strategies)\n",
    "# process_files(retail_base_path, \"retail\", missing_percentages, strategies)\n",
    "process_files(fps_base_path, \"fps\", missing_percentages, strategies)"
   ],
   "id": "c3910c0cba028cf5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/m20c7_k95j52q7xcg6lwyd7w0000gn/T/ipykernel_21403/762151888.py:4: DtypeWarning: Columns (25,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"{base_path}/NaN/{file_prefix}_{int(percentage * 100)}%.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/m20c7_k95j52q7xcg6lwyd7w0000gn/T/ipykernel_21403/762151888.py:4: DtypeWarning: Columns (25,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f\"{base_path}/NaN/{file_prefix}_{int(percentage * 100)}%.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 31\u001B[0m\n\u001B[1;32m     26\u001B[0m fps_base_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../../Data/Regression/fps in video games/Completeness\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Appeler la fonction pour chaque ensemble de données\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# process_files(heart_base_path, \"heart\", missing_percentages, strategies)\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# process_files(retail_base_path, \"retail\", missing_percentages, strategies)\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m process_files(fps_base_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfps\u001B[39m\u001B[38;5;124m\"\u001B[39m, missing_percentages, strategies)\n",
      "Cell \u001B[0;32mIn[13], line 8\u001B[0m, in \u001B[0;36mprocess_files\u001B[0;34m(base_path, file_prefix, missing_percentages, strategies)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Appliquer chaque stratégie d'imputation et sauvegarder les résultats\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m strategy_name, strategy_func, subfolder \u001B[38;5;129;01min\u001B[39;00m strategies:\n\u001B[0;32m----> 8\u001B[0m     df_imputed \u001B[38;5;241m=\u001B[39m impute_dataset(df, strategy_func)\n\u001B[1;32m      9\u001B[0m     df_imputed\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubfolder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(percentage\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m100\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[9], line 11\u001B[0m, in \u001B[0;36mimpute_dataset\u001B[0;34m(df, strategy)\u001B[0m\n\u001B[1;32m      9\u001B[0m     df_imputed \u001B[38;5;241m=\u001B[39m impute_numerical_by_median(df_imputed)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumerical\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mknn\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 11\u001B[0m     df_imputed \u001B[38;5;241m=\u001B[39m impute_numerical_by_knn(df_imputed)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumerical\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultiple\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     13\u001B[0m     df_imputed \u001B[38;5;241m=\u001B[39m impute_numerical_by_multiple(df_imputed)\n",
      "Cell \u001B[0;32mIn[8], line 27\u001B[0m, in \u001B[0;36mimpute_numerical_by_knn\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     25\u001B[0m df_imputed \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m     26\u001B[0m imputer \u001B[38;5;241m=\u001B[39m KNNImputer(n_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m df_imputed[df_imputed\u001B[38;5;241m.\u001B[39mselect_dtypes(include\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcolumns] \u001B[38;5;241m=\u001B[39m imputer\u001B[38;5;241m.\u001B[39mfit_transform(\n\u001B[1;32m     28\u001B[0m     df_imputed\u001B[38;5;241m.\u001B[39mselect_dtypes(include\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumber\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df_imputed\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/base.py:1098\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1083\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1084\u001B[0m             (\n\u001B[1;32m   1085\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) has a `transform`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1093\u001B[0m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[1;32m   1094\u001B[0m         )\n\u001B[1;32m   1096\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1097\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m-> 1098\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m   1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/impute/_knn.py:367\u001B[0m, in \u001B[0;36mKNNImputer.transform\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;66;03m# process in fixed-memory chunks\u001B[39;00m\n\u001B[1;32m    359\u001B[0m gen \u001B[38;5;241m=\u001B[39m pairwise_distances_chunked(\n\u001B[1;32m    360\u001B[0m     X[row_missing_idx, :],\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_X,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    365\u001B[0m     reduce_func\u001B[38;5;241m=\u001B[39mprocess_chunk,\n\u001B[1;32m    366\u001B[0m )\n\u001B[0;32m--> 367\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m gen:\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;66;03m# process_chunk modifies X in place. No return value.\u001B[39;00m\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_empty_features:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2172\u001B[0m, in \u001B[0;36mpairwise_distances_chunked\u001B[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[0m\n\u001B[1;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2171\u001B[0m     X_chunk \u001B[38;5;241m=\u001B[39m X[sl]\n\u001B[0;32m-> 2172\u001B[0m D_chunk \u001B[38;5;241m=\u001B[39m pairwise_distances(X_chunk, Y, metric\u001B[38;5;241m=\u001B[39mmetric, n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m   2173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (X \u001B[38;5;129;01mis\u001B[39;00m Y \u001B[38;5;129;01mor\u001B[39;00m Y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m   2174\u001B[0m     metric, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2175\u001B[0m ) \u001B[38;5;129;01mis\u001B[39;00m euclidean_distances:\n\u001B[1;32m   2176\u001B[0m     \u001B[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001B[39;00m\n\u001B[1;32m   2177\u001B[0m     \u001B[38;5;66;03m# i.e. \"l2\"\u001B[39;00m\n\u001B[1;32m   2178\u001B[0m     D_chunk\u001B[38;5;241m.\u001B[39mflat[sl\u001B[38;5;241m.\u001B[39mstart :: _num_samples(X) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2375\u001B[0m, in \u001B[0;36mpairwise_distances\u001B[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001B[0m\n\u001B[1;32m   2372\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m distance\u001B[38;5;241m.\u001B[39msquareform(distance\u001B[38;5;241m.\u001B[39mpdist(X, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n\u001B[1;32m   2373\u001B[0m     func \u001B[38;5;241m=\u001B[39m partial(distance\u001B[38;5;241m.\u001B[39mcdist, metric\u001B[38;5;241m=\u001B[39mmetric, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m-> 2375\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1893\u001B[0m, in \u001B[0;36m_parallel_pairwise\u001B[0;34m(X, Y, func, n_jobs, **kwds)\u001B[0m\n\u001B[1;32m   1890\u001B[0m X, Y, dtype \u001B[38;5;241m=\u001B[39m _return_float_dtype(X, Y)\n\u001B[1;32m   1892\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(n_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 1893\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(X, Y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m   1895\u001B[0m \u001B[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001B[39;00m\n\u001B[1;32m   1896\u001B[0m fd \u001B[38;5;241m=\u001B[39m delayed(_dist_wrapper)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Impact-of-data-quality-on-machine-learning-models/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:539\u001B[0m, in \u001B[0;36mnan_euclidean_distances\u001B[0;34m(X, Y, squared, missing_values, copy)\u001B[0m\n\u001B[1;32m    537\u001B[0m present_X \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m missing_X\n\u001B[1;32m    538\u001B[0m present_Y \u001B[38;5;241m=\u001B[39m present_X \u001B[38;5;28;01mif\u001B[39;00m Y \u001B[38;5;129;01mis\u001B[39;00m X \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m~\u001B[39mmissing_Y\n\u001B[0;32m--> 539\u001B[0m present_count \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(present_X, present_Y\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m    540\u001B[0m distances[present_count \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m    541\u001B[0m \u001B[38;5;66;03m# avoid divide by zero\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "  ",
   "id": "2d9d631bdbda6e9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
